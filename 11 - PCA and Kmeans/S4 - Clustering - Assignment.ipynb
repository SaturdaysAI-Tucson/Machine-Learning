{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mushroom Dataset\n",
    "\n",
    "You can get the data from:\n",
    "\n",
    "[Mushroom Dataset](https://www.kaggle.com/uciml/mushroom-classification)\n",
    "\n",
    "As you can see, there are a lot of variables, all categorical, thus scartterplot vizualizations won't work as in previuos cases.\n",
    "\n",
    "The variable we want to predict ``class`` is categorical, so you will not need to rescale the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load and read head of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decribe data, stdd, mean , etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information about each feauture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculat the number of NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# as we have done in the past, we need to find how many NaN for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look for weird values or elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new df with the cleanded data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer the value for missing data. You can use mode, mean, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at how many values exist per feature, do they all provide valuable information? if a feature don't add value, drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate variables between X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are trying to predict '''class'''\n",
    "y = \n",
    "X = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split, duh ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep this :)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a dataset where we have not seen anything yet! There is the problem that we have so many variables so it will be hard to do so... **PCA to the rescue**: we will apply PCa to reduce it to two dimensional space and we will print and see what are the **features that add value to the dataset**, the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca =       # use sklearn method\n",
    "pca.fit(X_train)\n",
    "\n",
    "# represent a scatterplot and add color tags to the training df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should look very spaced, at eyesight we can infer it :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try a classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# define the classificator and the number of stimators. \n",
    "# train the algorithm on the training data set.\n",
    "# obtain precission over the test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh sh*t, weird right??? go back and check if you partioned the predictor values correctly..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naaaah just kidding, that's fine, it is a simpel dataset and Random Forest is very good at it. Anyway, we need to check the dataset shape:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ufffffff too many features, right?  let's reduce them!\n",
    "How? PCA ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_features = # define number of values to test\n",
    "scores = []\n",
    "\n",
    "for n in n_features:\n",
    "    \n",
    "    # Perform PCA over X_train\n",
    "    # 1- Define PCA\n",
    "    # 2- Learn PCA over X_train\n",
    "    \n",
    "    # Train Random Forest\n",
    "    # 1- Difine RF\n",
    "    # 2- Train classifier\n",
    "    \n",
    "    # keep the score\n",
    "    \n",
    "    \n",
    "sns.lineplot(x=n_features, y=scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! we can see that after 10 feautures we get the score we were looking for and also we have reduce our variables on 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that the dataset is very simple, we can try to do some clusting on it and see what comes up.\n",
    "\n",
    "First step, is import Kmeans from sklearn, and then look for the optimal K, or optimal number of clusters. As we have seen before, we can obtain that value from the elbow on our graphic, that represtenst the total distance betweek the associated clusters. You can also - and should- look at the scikit learn library:\n",
    "\n",
    "[K-Means on sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "\n",
    "With this, we should be able to apply kmeans, evaluate and print our plot for the ``k's`` \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "scores = []\n",
    "k_values = # define a range\n",
    "for a in k_values:\n",
    "    \n",
    "    # Degine Kmeans and adjust\n",
    "    # Keep the prediction\n",
    "        \n",
    "sns.lineplot(x=k_values, y=scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the value obtained from the graphic, you can obtain a good approximation of Kmeans and with that explore how it segments them. We are going to use ``factorplot``, seaborn will do it. \n",
    "\n",
    "We are trying to see the distribution of the variable to predict given the cluster Kmeans detrminated it belonged to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run Kmeans with the obtained K \n",
    "\n",
    "kmeans = # defaine and train Kmeans\n",
    "\n",
    "# Prepare for factorplot\n",
    "\n",
    "\n",
    "# Plot\n",
    "ax = sns.factorplot(col=, x=, data=, kind='count',col_wrap=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how is that plot. For that, we will make a scatterplot but using the color for each cluster assigned on kmeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train PCA \n",
    "# Print the clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks very similar, right? well, not as good as the amazing RF, but we have been able to identify correctly the different datapoints in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
